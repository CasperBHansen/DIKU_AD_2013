%========== Dynamic Programming ==========%

\chapter{Dynamic Programming}
\label{ch:dynamicprog}

\textbf{Pensum} 15 \cite{clrs} \\\\
\textbf{Assignments} 15-2, 16-1 \\\\
\textbf{Algorithms} Rod-cutting, fibonacci \\\\
\textbf{Keywords} Time-memory trade-off, memoization, top-down / bottom-up
\vspace{1in}

\noindent There are two key characteristics that a problem must have for
dynamic programming to be a viable solution; optimal substructure and
overlapping subproblems.
\\\\
\noindent \textbf{Optimal Substructure}\\
% p. 379
We say that a problem exhibits optimal substructure, when an optimal solution
to the problem contains within it optimal solutions to subproblems. Generally,
in dynamic programming we use this property to build an optimal solution to
the original problem from optimal solutions to its subproblems.
\\\\
\noindent \textbf{Overlapping Subproblems}\\
% p. 384 - TODO: make this more concise
When a recursive algorithm revisits the same problem repeatedly, we say that
the optimization problem has \textit{overlapping subproblems}. That is, we
solve the same subproblems, rather than generating new subproblems. In
contrast to the divide and conquer technique, where we generate and solve new
subproblems that are smaller instances of the same type of problem, in dynamic
programming we solve each subproblem once, and then storing the solution in a
table, where it can be looked up when needed in $\Theta(1)$ time.

\newpage
\section{Discovering Optimal Substructure}
% p. 379-380
\begin{enumerate}
	\item Show that a solution to the problem consists of making a choice.
Making this choice leaves one or more subproblems to be solved.
	\item Suppose that for a given problem there exists a choice which leads
to an optimal substructure. Do not concern yourself with how to determine this
choice, but assume it has been given to you.
	\item Given this choice, determine which subproblems ensue and how to best
characterize the resulting space of subproblems.
	\item Using a "cut-and-paste" technique, show that the solutions to the
subproblems must themselves be optimal. You can do so by supposing that each
subproblem is not optimal and then deriving a contradiction.

The "cut-and-paste" refers to cutting out the nonoptimal solution for each of
the subproblems, and pasting in the optimal one. Doing so shows that for the
original problems solution to be optimal, so must its subproblems.

Should an optimal solution give rise to more than one subproblem, they are
typically so similar that you can modify the cut-and-paste argument for one to
apply to the others with little effort.
\end{enumerate}

\newpage
\section{Methods of Approach}
There are usually two equivalent ways to implement a dynamic-programming
approach; top-down with memoization and bottom-up.

\subsection{Top-down with Memoization}
% taken directly from the CLRS book, good summary I think.
In this approach, we write the procedure recursively in a natural manner, but
modified to save the result of each subproblem (usually in an array or hash
table). The procedure now first checks to see whether it has previously solved
this problem. If so, it returns the saved value, saving further computation at
this level; if not, the procedure computes the value in the usual manner. We
say that the recursive procedure has been \textit{memoized}; it "remembers"
what results it has computed previously.

The caveat of using the top-down approach is that we make time-memory
trade-off, so what we gain in performance is payed in memory usage instead.

\subsection{Bottom-up}
% taken directly from the CLRS book, good summary I think.
This approach typically depends on some natural notion of the "size" of a
subproblem, such that solving any particular subproblem depends only on
solving "smaller" subproblems.

We sort the subproblems by size and solve them in size order, smallest first.
When solving a particular subproblem, we have already solved all of the
smaller subproblems its solution depends upon, and we saved their solutions.
We solve each subproblem only once, and when we first see it, we have already
solved all of its prerequisite subproblems.

\newpage
\section{Example}
Consider the rod-cutting problem; given a rod of length $n$ inches and a table
of prices $p_i$, for $i = 1, 2, \dots, n$, determine the maximum revenue $r_n$
obtainable by cutting up the rod and selling the pieces.
\\\\
We give the recursive -and inefficient- algorithm to solve this problem. \\\\
\begin{algorithm}[H]
	\caption{Recursive rod-cutting procedure}
	\label{alg:cut-rod|type:recursive}
	
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetKwFunction{CutRod}{CutRod}
	
	\Input{A price table $p$ and a rod-length $n$.}
	\Output{The best possible price for a rod of length $n$.}
	
	\BlankLine
	\CutRod($p$, $n$) \\
	\Begin
	{
		\If{$n == 0$}
		{
			\Return $0$
		}
		$q = -\infty$ \\
		\For{$i = 1$ \KwTo $n$}
		{
			$q = max(q, p[i] + $ \CutRod($p$, $n-i$) $)$
		}
		\Return $q$
	}
\end{algorithm}
The reason this algorithm is inefficient, is because it calculates the same
inputs many times. We can improve algorithm \ref{alg:cut-rod|type:recursive}
using dynamic programming.

\newpage
\subsection{Top-down Rod-cutting}
The main difference in this approach is that we store all inputs that we
haven't computed already. \\\\
\begin{algorithm}[H]
	\caption{Top-down rod-cutting procedure}
	\label{alg:cut-rod|type:memoized}
	
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetKwFunction{MemoizedCutRod}{MemoizedCutRod}
	\SetKwFunction{MemoizedCutRodAux}{MemoizedCutRodAux}
	
	\Input{A price table $p$ and a rod-length $n$.}
	\Output{The best possible price for a rod of length $n$.}
	
	\BlankLine
	\MemoizedCutRod($p$, $n$) \\
	\Begin
	{
		Let $r[0 \dots n]$ be a new array \\
		\For{$i = 1$ \KwTo $n$}
		{
			$r[i] = -\infty$
		}
		\Return \MemoizedCutRodAux($p$, $n$, $r$)
	}
\end{algorithm}
The actual work is done by the auxiliary procedure, which is basically the
same, with minor differences compared to the recursive procedure. \\\\
\begin{algorithm}[H]
	\caption{Top-down rod-cutting auxiliary procedure}
	\label{alg:cut-rod|type:auxiliary}
	
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetKwFunction{MemoizedCutRodAux}{MemoizedCutRodAux}
	
	\Input{A price table $p$, a rod-length $n$ and a memoization table $r$.}
	\Output{The best possible price for a rod of length $n$.}
	
	\BlankLine
	\MemoizedCutRodAux($p$, $n$, $r$) \\
	\Begin
	{
		\If{$r[n] \geq 0$}{ \Return $r[n]$ }
		\eIf{$n == 0$}{ $q = 0$ }{ $q = -\infty$ }
		\For{$i = 1$ \KwTo $n$}
		{
			$q = max(q, p[i] + $ \MemoizedCutRodAux($p$, $n-i$, $r$) $)$
		}
		$r[n] = q$ \\
		\Return $q$
	}
\end{algorithm}

\subsection{Bottom-up Rod-cutting}
The bottom-up procedure solves the problem in the order of smaller subproblems
first. It does so by maintaining two figurative pointers of the rod and
continually replacing the previously found $r[i]$, should it to find a better
solution for the next iteration. \\\\
\begin{algorithm}[H]
	\caption{Bottom-up rod-cutting procedure}
	\label{alg:cut-rod|type:bottom-up}
	
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetKwFunction{BottomUpCutRod}{BottomUpCutRod}
	
	\Input{A price table $p$ and a rod-length $n$.}
	\Output{The best possible price for a rod of length $n$.}
	
	\BlankLine
	\BottomUpCutRod($p$, $n$) \\
	\Begin
	{
		Let $r[0 \dots n]$ be a new array \\
		$r[0] = 0$ \\
		\For{$i = 1$ \KwTo $n$}
		{
			$q = -\infty$
			\For{$j = 1$ \KwTo $i$}
			{
				$q = max(q, p[j] + r[i - j])$
			}
			$r[i] = q$
		}
		\Return $r[n]$
	}
\end{algorithm}
...
