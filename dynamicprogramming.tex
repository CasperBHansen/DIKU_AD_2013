%========== Dynamic Programming ==========%

\chapter{Dynamic Programming}
\label{ch:dynamicprogramming}

\textbf{Pensum} 15 \cite{clrs} \\\\
\textbf{Assignments} 15-2, 16-1 \\\\
\textbf{Algorithms} Rod-cutting, fibonacci \\\\
\textbf{Keywords} Time-memory trade-off, memoization, top-down / bottom-up
\vspace{1in}

\noindent There are two key characteristics that a problem must have for
dynamic programming to be a viable solution; optimal substructure and
overlapping subproblems.
\\\\
\noindent \textbf{Optimal Substructure}\\
% p. 379
We say that a problem exhibits optimal substructure, when an optimal solution
to the problem contains within it optimal solutions to subproblems. We build
a solution for a problem from optimal solutions to its subproblems.
\\\\
\noindent \textbf{Overlapping Subproblems}\\
% p. 384 - TODO: make this more concise
When a recursive algorithm revisits the same problem repeatedly, we say that
the optimization problem has \textit{overlapping subproblems}. That is, we
solve the same subproblems, rather than generating new subproblems. % In
% contrast to the divide and conquer technique, where we generate and solve new
% subproblems that are smaller instances of the same type of problem, in dynamic
% programming we solve each subproblem once, and then storing the solution in a
% table, where it can be looked up when needed in $\Theta(1)$ time.

\newpage
\section{Discovering Optimal Substructure}
% p. 379-380
\begin{enumerate}
	\item Show that a solution to the problem consists of making a choice.
Making this choice leaves one or more subproblems to be solved.
	\item Suppose that for a given problem there exists a choice which leads
to an optimal substructure. Do not concern yourself with how to determine this
choice, but assume it has been given to you.
	\item Given this choice, determine which subproblems ensue and how to best
characterize the resulting space of subproblems.
	\item Using a "cut-and-paste" technique, show that the solutions to the
subproblems must themselves be optimal. You can do so by supposing that each
subproblem is not optimal and then deriving a contradiction.
\end{enumerate}

\section{Methods of Approach}
There are usually two equivalent ways to implement a dynamic-programming
approach; top-down with memoization and bottom-up.

\subsection{Top-down with Memoization}
% taken directly from the CLRS book, good summary I think.
In this approach, we write the procedure recursively in a natural manner, but
modified to save the result of each subproblem. The procedure checks to see
if it has already solved this problem. If so, it returns a saved value, saving
further computation at this level; if not, the procedure computes the value in
the usual manner. We say that the recursive procedure has been
\textit{memoized}; it "remembers" what results it has computed previously.

The caveat of using the top-down approach is that we make time-memory
trade-off, so what we gain in performance is payed in memory usage instead.

\subsection{Bottom-up}
% taken directly from the CLRS book, good summary I think.
This approach typically depends on some natural notion of the "size" of a
subproblem, such that solving any particular subproblem depends only on
solving "smaller" subproblems.

We sort the subproblems by size and solve them in size order, smallest first.
When solving a particular subproblem, we have already solved all of the
smaller subproblems its solution depends upon, and we saved their solutions.
We solve each subproblem only once, and when we first see it, we have already
solved all of its prerequisite subproblems.

\newpage
\section{Longest Common Subsequence}
% p. 390-396, CLRS
In the longest common subsequence problem we are given two sequences $X =
\langle x_1, x_2, \dots, x_m \rangle$ and $Y = \langle y_1, y_2, \dots, y_n
\rangle$, and we wish to find a subsequence of both $X$ and $Y$ of maximum
length.

\subsection{Optimal Substructure}
The brute-force approach would require exponential time, since the number of
subsequences of $X$ is $2^m$, making it impractical for large inputs.

The LCS-problem exhibits the optimal substructure property, however, which is
shown in the following theorem. First, we define the \textit{i}th prefix by
$Z_i = \langle z_1, z_2, \dots, z_i \rangle$, and $Z_0$ is the empty sequence.

\begin{theorem} \cite[thm. 15.1, p. 392]{clrs} \\
\label{thm:lcs-optimal-substructure}
	Let $X = \langle x_1, x_2, \dots, x_m \rangle$
	and $Y = \langle y_1, y_2, \dots, y_n \rangle$
	be sequences, and let $Z = \langle z_1, z_2, \dots, z_k \rangle$ be any
	LCS of $X$ and $Y$.
	\textnormal
	{
	\begin{enumerate}
		\item If $x_m = y_m$, then $z_k = x_m = y_n$ and $Z_{k-1}$ is an
LCS of $X_{m-1}$ and $Y_{n-1}$.
		\item If $x_m \neq y_n$, then $z_k \neq x_m$ implies that $Z$ is an
LCS of $X_{m-1}$ and $Y$.
		\item If $x_m \neq y_n$, then $z_k \neq y_n$ implies that $Z$ is an
LCS of $X$ and $Y_{n-1}$.
	\end{enumerate}
	}
\end{theorem}

% proof ?

\subsection{Overlapping subproblems}
Theorem \ref{thm:lcs-optimal-substructure} gives us two recursive cases; one
of which leaves us one subproblem, and the other two subproblems, which shows
that the  LCS problem has overlapping subproblems.

We define value (length of the subsequence) of the recursive cases by the
table entry $c[i,j]$. This gives us a way to express the recursion.

\begin{align}
	c[i,j] =
	\begin{cases}
		0 & \mbox{if $i = 0$ or $j = 0$} \\
		c[i - 1, j - 1] + 1 & \mbox{if $i, j > 0$ and $x_i = y_j$} \\
		max(c[i, j-1], c[i-1, j]) & \mbox{if $i, j > 0$ and $x_i \neq y_j$}
	\end{cases}
\end{align}

\newpage
\subsection{Bottom-up solution}
...
\\\\
\begin{algorithm}[H]
	\caption{Longest common subsequence, bottom-up.}
	\label{alg:lcs-bottom-up}
	
	\SetKwInOut{In}{Input}
	\SetKwInOut{Out}{Output}
	
	\SetKwFunction{LCS}{LCS}
	
	\In{Takes two sequences $X$ and $Y$.}
	\Out{A longest common subsequence of $X$ and $Y$.}
	
	\BlankLine
	\LCS($X$, $Y$) \\
	\Begin
	{
		% initialization
		$m = X.length$ \\
		$n = Y.length$ \\
		Let $c[0 \dots m, 0 \dots n]$ be a new table \\
		\For{$i = 1$ \KwTo $m$}
		{
			$c[i, 0] = 0$
		}
		\For{$j = 1$ \KwTo $n$}
		{
			$c[0, i] = 0$
		}
		\For{$i = 1$ \KwTo $m$}
		{
			\For{$j = 1$ \KwTo $n$}
			{
				\uIf{$x_i == y_i$}
				{
					$c[i, j] = c[i-1,j-1] + 1]$ \\
					$b[i, j] ={\ }\nwarrow$
				}
				\uElseIf{$c[i-1, j] \geq c[i, j-1]$}
				{
					$c[i, j] = c[i-1, j]$ \\
					$b[i, j] ={\ }\uparrow$
				}
				\Else
				{
					$c[i, j] = c[i, j-1]$ \\
					$b[i, j] ={\ }\leftarrow$
				}
			}
		}
		\Return $b$ and $c$
	}
\end{algorithm}

\newpage % NOTE: obsolete when LCS has been added
\section{Rod-cutting}
Consider the rod-cutting problem; given a rod of length $n$ inches and a table
of prices $p_i$, for $i = 1, 2, \dots, n$, determine the maximum revenue $r_n$
obtainable by cutting up the rod and selling the pieces.
\\\\
We give the recursive -and inefficient- algorithm to solve this problem. \\\\
\begin{algorithm}[H]
	\caption{Recursive rod-cutting procedure}
	\label{alg:cut-rod|type:recursive}
	
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetKwFunction{CutRod}{CutRod}
	
	\Input{A price table $p$ and a rod-length $n$.}
	\Output{The best possible price for a rod of length $n$.}
	
	\BlankLine
	\CutRod($p$, $n$) \\
	\Begin
	{
		\If{$n == 0$}
		{
			\Return $0$
		}
		$q = -\infty$ \\
		\For{$i = 1$ \KwTo $n$}
		{
			$q = max(q, p[i] + $ \CutRod($p$, $n-i$) $)$
		}
		\Return $q$
	}
\end{algorithm}
The reason this algorithm is inefficient, is because it calculates the same
inputs many times. We can improve algorithm \ref{alg:cut-rod|type:recursive}
using dynamic programming.

\newpage
\subsection{Top-down Rod-cutting}
The main difference in this approach is that we store all inputs that we
haven't computed already. \\\\
\begin{algorithm}[H]
	\caption{Top-down rod-cutting procedure}
	\label{alg:cut-rod|type:memoized}
	
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetKwFunction{MemoizedCutRod}{MemoizedCutRod}
	\SetKwFunction{MemoizedCutRodAux}{MemoizedCutRodAux}
	
	\Input{A price table $p$ and a rod-length $n$.}
	\Output{The best possible price for a rod of length $n$.}
	
	\BlankLine
	\MemoizedCutRod($p$, $n$) \\
	\Begin
	{
		Let $r[0 \dots n]$ be a new array \\
		\For{$i = 1$ \KwTo $n$}
		{
			$r[i] = -\infty$
		}
		\Return \MemoizedCutRodAux($p$, $n$, $r$)
	}
\end{algorithm}
The actual work is done by the auxiliary procedure, which is basically the
same, with minor differences compared to the recursive procedure. \\\\
\begin{algorithm}[H]
	\caption{Top-down rod-cutting auxiliary procedure}
	\label{alg:cut-rod|type:auxiliary}
	
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetKwFunction{MemoizedCutRodAux}{MemoizedCutRodAux}
	
	\Input{A price table $p$, a rod-length $n$ and a memoization table $r$.}
	\Output{The best possible price for a rod of length $n$.}
	
	\BlankLine
	\MemoizedCutRodAux($p$, $n$, $r$) \\
	\Begin
	{
		\If{$r[n] \geq 0$}{ \Return $r[n]$ }
		\eIf{$n == 0$}{ $q = 0$ }{ $q = -\infty$ }
		\For{$i = 1$ \KwTo $n$}
		{
			$q = max(q, p[i] + $ \MemoizedCutRodAux($p$, $n-i$, $r$) $)$
		}
		$r[n] = q$ \\
		\Return $q$
	}
\end{algorithm}

\subsection{Bottom-up Rod-cutting}
The bottom-up procedure solves the problem in the order of smaller subproblems
first. It does so by maintaining two figurative pointers of the rod and
continually replacing the previously found $r[i]$, should it to find a better
solution for the next iteration. \\\\
\begin{algorithm}[H]
	\caption{Bottom-up rod-cutting procedure}
	\label{alg:cut-rod|type:bottom-up}
	
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetKwFunction{BottomUpCutRod}{BottomUpCutRod}
	
	\Input{A price table $p$ and a rod-length $n$.}
	\Output{The best possible price for a rod of length $n$.}
	
	\BlankLine
	\BottomUpCutRod($p$, $n$) \\
	\Begin
	{
		Let $r[0 \dots n]$ be a new array \\
		$r[0] = 0$ \\
		\For{$i = 1$ \KwTo $n$}
		{
			$q = -\infty$
			\For{$j = 1$ \KwTo $i$}
			{
				$q = max(q, p[j] + r[i - j])$
			}
			$r[i] = q$
		}
		\Return $r[n]$
	}
\end{algorithm}
...
