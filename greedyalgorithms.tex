%========== Greedy Algorithms ==========%

\chapter{Greedy Algorithms}
\label{ch:greedyalgorithms}

\textbf{Pensum} 16 \cite{clrs} \\\\
\textbf{Assignments} 16-1 \\\\
\textbf{Algorithms} Activity selection, huffman trees \\\\
\textbf{Keywords} The greedy choice, local- and global optimal solution
\vspace{1in}

\noindent Greedy algorithms follow an algorithmic methodology that makes a
series of choices under the assumption that the choice that seems best at the
moment (a \textit{locally optimal solution}) will produce \textit{globally
optimal solution} - this isn't always the case, but in some cases we can prove
that it does.
\\\\
\noindent \textbf{Dynamic Programming} \\
Although greedy problems often exhibit the same characteristics as a dynamic
programming problem, the key difference between these two algorithmic
techniques is that dynamic programming problems tries all possible solutions,
and hence always yields an optimal solution. This is not necessarily the case
for a greedy algorithm, as it does not concern itself with the subproblem, but
rather makes a locally optimal choice \textit{in the hope} that this will lead
to a globally optimal solution - we can, however, show that it will for some
problems.

\newpage
\section{The Greedy Strategy}
Here we list the procedure one must follow in order to design a greedy
algorithm. Each point will be discussed in depth under its own section.
\begin{enumerate}
	\item \textbf{Describe} the optimization problem as one in which making a
choice, we are left with one subproblem.
	\item \textbf{Prove} that the greedy choice always yields a globally
optimal solution.
	\item \textbf{Demonstrate} optimal substructure by showing that, having
made a greedy choice, what remains is a subproblem with the property that if
we combine an optimal solution with the greedy greedy choice, we arrive at an
optimal solution to the original problem.
\end{enumerate}

\subsection{Describing the Optimization Problem}
% p. ?-?, CLRS
...

\subsection{Proving The Greedy Choice}
% p. 424, CLRS
As the greedy choice is not always the optimal choice, you will have to prove
the choice for each algorithme. The proof examines a globally optimal solution
to some subproblem. It then shows how to modify the solution to substitute the
greedy choice for some other choice, resulting in one similar, but smaller,
subproblem.

\subsection{Demonstrating Optimal Substructure}
% p. ?-?, CLRS
...

\clearpage
\section{Huffman Codes}
% p. 428-435, CLRS
The Huffman code algorithm compresses data by constructing a tree-structure
from the data by means of the frequency of occurrences within the data.
\\\\
\begin{algorithm}[H]
	\SetKwInOut{In}{Input}
	\SetKwInOut{Out}{Output}
	
	\SetKwFunction{HuffmanCode}{HuffmanCode}
	\SetKwFunction{ExtractMin}{ExtractMin}
	\SetKwFunction{Insert}{Insert}
	
	\In{A data set $C$, each node within it has an attribute $freq$.}
	\Out{The root node of the tree-structure built.}
	\BlankLine
	
	\HuffmanCode($C$) \\
	\Begin
	{
		$n = |C|$ \\
		$Q = C$ \\
		\For{$i = 1$ \KwTo $n - 1$}
		{
			allocate a new node $z$ \\
			$z.left = x = $ \ExtractMin($Q$) \\
			$z.right = y = $ \ExtractMin($Q$) \\
			$z.freq = z = x.freq + y.freq$ \\
			\Insert($Q$, $z$)
		}
		\Return \ExtractMin($Q$)
	}
\end{algorithm}

\subsection{Analysis}
During the initialization it sets $n$ to the cardinality of the set $C$ on
line 3, and builds a min-heap priority queue from the set $C$, keyed on the
frequency of the nodes, on line 4. The latter taking the majority of the
running-time of the initialization at $O(n)$ (see algorithm
\ref{alg:build-max-heap} on page ~\ref{alg:build-max-heap}).

The loop in lines 5--11 runs exactly $n-1$ times, executing heap operations
that take $O(\lg n)$ each, and so it contributes a total of $O(n \lg n)$.

% \subsection{Proof}
% We must first show that the problem exhibits optimal substructure, and that
% the greedy choice is always right. First we show that the greedy choice holds.

% \begin{lemma}
	
% \end{lemma}

