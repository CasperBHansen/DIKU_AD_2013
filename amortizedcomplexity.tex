%========== Amortized Complexity ==========%

\chapter{Amortized Complexity}
\label{ch:amortizedcomplexity}

\textbf{Pensum} 17 \cite{clrs} \\\\
\textbf{Assignments} 7-1 (quicksort) \\\\
\textbf{Algorithms} Table doubling, quicksort \\\\
\textbf{Keywords} Aggregate analysis, accounting- and potential method, average.
\vspace{1in}

\noindent In amortized complexity analysis, we average the time required to
perform a sequence of data-structure operations over all the operations
performed. With amortized analysis, we can show that the average cost of an
operation is small, if we average it over a sequence of operations, even
though a single operation within the sequence might be expensive.
\\\\
\noindent \textbf{Amortized- and Average-case Analysis} \\
Amortized analysis differs from average-case analysis in that probability is
not involved; an amortized analysis guarantees the \textit{average performance
of each operation in the worst case}.

\newpage
\section{Aggregate Analysis}
In aggregate analysis, we show that for all $n$, a sequence of $n$ operations
takes \textit{worst-case} time $T(n)$ in total. The average cost, or \textbf{
amortized cost}, however, per operation is $\frac{T(n)}{n}$. The amortized
cost applies to each operation, even when there are several types of
operations in the sequence.

\subsection{Example}
Consider a dynamic table, which doubles in size whenever it runs out of empty
space. Let us examine a sequence of $n$ operations, on an initially empty
table. While the table has empty spaces, we simply insert new items, each has
a cost of 1. If the table has no more room, however, we perform an expansion,
and we have to copy all of the previous elements along with the insertion,
which gives us a cost of $c_i = i$.

Performing $n$ operations thus gives us an upper-bound of $O(n^2)$, but
because of the rare expansion, this bound is not tight. Specifically, this
only happens when $i - 1$ is an exact power of 2.
\begin{align}
	c_i =
	\begin{cases}
		i & \mbox{if $i - 1$ is an exact power of two} \\
		1 & \mbox{otherwise}
	\end{cases}
\end{align}
We can tighten this bound using aggregate analysis.
\begin{align}
	\sum_{i=1}^{n}c_i
	\leq n \sum_{j=0}^{\lfloor \lg n \rfloor} 2^j
	< n + 2n
	= 3n
\end{align}
Reasoned by the fact that at most $n$ operations cost 1, and the remaining
form a geometric series. Since $n$ operations are bounded by $3n$, the
amortized cost per operation is $3$.

\newpage
\section{Accounting Method}
In the accounting method of amortized complexity analysis, we \textit{assign}
a cost to each operation - this amortized cost does not necessarily reflect
the actual cost of an operation. When an operation's amortized cost exceeds
its actual cost, we perceive this as deposited credit that we can use to pay
for subsequent operations that might be undercharged, that is an operation
whose amortized cost is less than its actual cost.

Although this method is very similar to \textit{aggregate analysis}, the key
difference between these is that in aggregate analysis all operations have
the same amortized cost, whereas in the accounting method we may assign
different amortized cost for different operations.

It follows naturally in aggregate analysis that the total amortized cost of
$n$ operations provides an upper bound on the total actual cost of the
sequence, but for the accounting method we must define this explicitly.
\begin{align}
\label{ch:amortizedcomplexity|eqn:costrelationship}
	\sum_{i=1}^{n}{\hat{c_i}} \geq \sum_{i=1}^{n}{c_i}
	\quad \text{, where }c_i\text{ is the actual cost, and }
	\hat{c_i}\text{ is the amortized cost.}
\end{align}
This relationship, as described by the above equation, must hold for all
sequences of operations. It follows from equation
(\ref{ch:amortizedcomplexity|eqn:costrelationship}) that the difference
between the total actual cost must never exceed that of the total amortized
cost, which would result in negative credit and this does not provide an upper
bound, and thus not a guarantee of the average performance of each operation
in the worst case either. In other words, we can always pay in advance, but
we cannot borrow.

\subsection{Example}
Consider an array $A$ of length $n$. Appending an element onto the array
requires linear time $O(n)$ as we must reallocate a new array $B$ of length
$n+1$ and copy all $n$ the elements of $A$ into $B$ and put the new element
at the end of $B$. This isn't very efficient, at all. We can do a lot better.
\\\\
Let's assume that the array $A$ is of length $n$ and that it initially holds
\texttt{nil} in all $n$ slots, and for convenience we maintain a pointer $p$
to the last non-\texttt{nil} element of the array. Then we adding an element
to the array $A$ is constant for $p < n$, as it is simply a matter of putting
the element into the $p$th slot of $A$ and incrementing the pointer - and
these are both constant time operations. Now, for $p = n$ we must then
reallocate the array, as is the case for the previous explanation, but when
we do, we allocate $B$ with twice the length of $A$. This operation takes
$O(n)$ as before, but notice that this will occur less frequently as $n$ grows
larger.
\\\\
\noindent \textbf{Analysis}
Over a sequence of $n$ append operations we spend $n - 1$ constant time
operations where we simply put the element in the right slot and increment
the pointer. We then spend $O(n)$ time copying the contents of $A$ over into
$B$.
\\\\
\noindent We assign the following amortized costs for the operation.
\begin{align}
	\texttt{Append}
	\begin{cases}
		3 &\mbox{for } p < n \\
		0 &\mbox{for } p = n
	\end{cases}
\end{align}
The majority of the work is done when $p = n$, but we want to even out this
work throughout the entire sequence of operations, hence we make the case
where $p < n$ pay for 1 itself, and 1 for both the copying procedure and for
refilling the deposited credit after the copy phase, which amounts to a grand
total of 3 for each operation where $p < n$.

% possibly add figures showing this ?

As such, the amortized cost spent on each operation is constant, which gives
us the upper bound on the algorithm of $\Theta(1)$.

\section{Potential Method}
Using the potential method is a lot more rigorous than the two previously
mentioned methodologies, as it is firmly rooted in mathematics all the way
throughout the proof. The potential method is based on the same notion as the
accounting method, the difference being the way how we perceive the problem of
amortization. In the potential method we derive the amortized cost from the
datastructure as we analyze the potential it holds mapped onto it as it
changes from one iteration to the next, as opposed to the accounting method,
where we make an guess about the amortized cost and argue its correctness
after the fact.
\\\\
We define the potential function $\Phi(D_i) \rightarrow \mathbb{R}$ as a
mapping of the potential of the datastructure $D$ at a particular iteration
$i$ onto the real numbers. We must maintain the invariant that the initial
potential of any datastructure is zero, that is $\Phi(D_0) = 0$, and that
subsequent iterations on the datastructure is always non-negative, that is
$\Phi(D_i) \geq 0$.

As we can now describe the potential at any given iteration, we can express
the potential difference between two such iterations as $\Delta\Phi_i =
\Phi(D_i) - \Phi(D_{i-1})$. This gives us a tool to analyze how the
potential changes.

We define the amortized cost $\hat{c_i}$ such that it provides an upper bound
on the actual cost $c_i$ by means of the potential difference,
\begin{align}
	\hat{c_i} = c_i + \Phi(D_i) - \Phi(D_{i-1})
\end{align}
which gives us an invariant that we must maintain. That is, whenever the
actual cost $c_i$ exceeds that of the amortized cost $\hat{c_i}$, the
potential difference must be able to make up for this. To define this more
clearly, we state the cases that
\begin{align}
	\Delta\Phi_i
	\begin{cases}
		\Delta\Phi_i > 0 \text{, then }c_i < \hat{c_i} &
		\text{Excess potential, stored for later.} \\
		\Delta\Phi_i < 0 \text{, then }c_i > \hat{c_i} &
		\text{Insufficient potential, use stored.}
	\end{cases}	
\end{align}
which is somewhat parallel to the accounting method, except we are expressing
the amortized cost $\hat{c_i}$ by means of the potential difference, rather
than an arbitrary guess.

Substituting these findings into the equation that defines the upper bound of
the amortization on the actual cost, we prove its correctness.
\begin{align}
	\sum_{i=1}^{n}{\hat{c_i}}
	&= \sum_{i=1}^{n}{(c_i + \Phi(D_i) - \Phi(D_{i-1}))} \\
	&= \sum_{i=1}^{n}{c_i + \Phi(D_n) - \Phi(D_{0})} \\
	&= \sum_{i=1}^{n}{c_i + \Phi(D_n)} \\
	&\geq \sum_{i=1}^{n}{c_i}
\end{align}
Notice that we make use of the rule of telescoping sums (see appendix
\ref{appendix:equations|eqn:telescoping}). We note that $\Phi(D_0) = 0$,
and that $\Phi(D_n) \geq 0$, proving that the amortized cost, as computed in
the potential method, provides an upper bound on the actual cost.

\newpage
\subsection{Example}
Again, using the table-expansion problem, we define a potential function
$\Phi$ that is 0 immediately following an expansion, but gradually builds to
table-size as we fill it up.
\begin{align}
	\Phi(T) = 2 \cdot T.num - T.size
\end{align}
The above is a possibility as after an expansion we have that $T.num =
T.size/2$, and thus $\Phi(T) = 0$. Also, right before an expansion we have
that $T.num = T.size$, and thus $\Phi(T) = T.num$. The initial value is 0,
and since the table is always at least half full, $T.num \geq T.size/2$,
which implies that $\Phi(T)$ is always nonnegative. Given these properties,
the potential function must provide an upper bound on the actual cost.

Let $num_i$ denote the number of items stored in the table after the $i$th
operation, and likewise let $size_i$ denote the size of the table after the
$i$th operation.

Then we have for operations that does not trigger an expansion that
\begin{align}
	\hat{c_i} &= c_i + \Phi_i - \Phi_{i-1} \\
	&= 1 + (2 \cdot num_i - size_i) - (2 \cdot num_{i-1} - size_{i-1}) \\
	&= 1 + (2 \cdot num_i - size_i) - (2 \cdot (num_{i}-1) - size_{i}) \\
	&= 3
\end{align}
Likewise, for operations that do trigger an expansion, we have that
\begin{align}
	\hat{c_i} &= c_i + \Phi_i - \Phi_{i-1} \\
	&= num_i + (2 \cdot num_i - size_i) - (2 \cdot num_{i-1} - size_{i-1}) \\
	&= num_i + (2 \cdot num_i - 2 \cdot (num_i - 1)
	- (2 \cdot (num_{i} - 1) - (num_i - 1)) \\
	&= num_i + 2 - (num_i - 1) \\
	&= 3
\end{align}
And thus, we have shown that the amortized cost for both cases is 3.

